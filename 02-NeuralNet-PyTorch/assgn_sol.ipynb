{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Name: Shubhajeet Das <br />\n",
        "Roll No.: 24AI10013 <br />\n",
        "DL Assignment Day 2"
      ],
      "metadata": {
        "id": "VWqaP-oGqk-V"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kGUw-YxHqCmR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "def set_all_seeds(seed):\n",
        "    \"\"\"Sets the seed for multiple libraries to ensure reproducibility.\"\"\"\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "set_all_seeds(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problem Statement\n",
        "You are given properties of wine samples.\n",
        "Your task is to predict wine quality (score from 0 to 10) using an Artificial Neural Network (ANN).\n",
        "\n",
        "This is a regression problem, not classification."
      ],
      "metadata": {
        "id": "3J3EZTHbq6V0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Collection & Exploration (2 marks)"
      ],
      "metadata": {
        "id": "es-MHNbyrAY8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the Dataset (1 mark)\n",
        "\n",
        "Download the [Wine Quality](https://archive.ics.uci.edu/dataset/109/wine) - Red dataset (CSV)\n",
        "\n",
        "Load it using pandas\n",
        "\n",
        "Print:\n",
        "- Dataset shape\n",
        "- First 5 rows\n",
        "- Summary statistics"
      ],
      "metadata": {
        "id": "eIJ_IJwrrEUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ucimlrepo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_gBthCHrKjU",
        "outputId": "034a18cc-9cce-4b4b-90ea-014362b662e5"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ucimlrepo in /usr/local/lib/python3.12/dist-packages (0.0.7)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ucimlrepo) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2020.12.5 in /usr/local/lib/python3.12/dist-packages (from ucimlrepo) (2026.1.4)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->ucimlrepo) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->ucimlrepo) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->ucimlrepo) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from ucimlrepo import fetch_ucirepo"
      ],
      "metadata": {
        "id": "H5F3yUbkqdQ_"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature-Target Split (1 mark)\n",
        "\n",
        "Separate:\n",
        "\n",
        "Input features X and target variable y\n",
        "\n",
        "Convert both into PyTorch tensors"
      ],
      "metadata": {
        "id": "1gf_hxRfrU9E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "wine = fetch_ucirepo(id=109)\n",
        "X = wine.data.features\n",
        "y = wine.data.targets\n",
        "df = pd.concat([X, y], axis=1)"
      ],
      "metadata": {
        "id": "Omtm0H3vrPaO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Dataset Shape:\", df.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fvGM2An-vwQM",
        "outputId": "88e499b4-2307-48c7-afbd-fad9fa453f9d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Shape: (178, 14)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "BG2lm-1vv0Uc",
        "outputId": "53ef336d-30a1-48ed-879b-ff9785f47e00"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   Alcohol  Malicacid   Ash  Alcalinity_of_ash  Magnesium  Total_phenols  \\\n",
              "0    14.23       1.71  2.43               15.6        127           2.80   \n",
              "1    13.20       1.78  2.14               11.2        100           2.65   \n",
              "2    13.16       2.36  2.67               18.6        101           2.80   \n",
              "3    14.37       1.95  2.50               16.8        113           3.85   \n",
              "4    13.24       2.59  2.87               21.0        118           2.80   \n",
              "\n",
              "   Flavanoids  Nonflavanoid_phenols  Proanthocyanins  Color_intensity   Hue  \\\n",
              "0        3.06                  0.28             2.29             5.64  1.04   \n",
              "1        2.76                  0.26             1.28             4.38  1.05   \n",
              "2        3.24                  0.30             2.81             5.68  1.03   \n",
              "3        3.49                  0.24             2.18             7.80  0.86   \n",
              "4        2.69                  0.39             1.82             4.32  1.04   \n",
              "\n",
              "   0D280_0D315_of_diluted_wines  Proline  class  \n",
              "0                          3.92     1065      1  \n",
              "1                          3.40     1050      1  \n",
              "2                          3.17     1185      1  \n",
              "3                          3.45     1480      1  \n",
              "4                          2.93      735      1  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-7168aa7c-1364-4e8c-93d6-59ab2b725e8c\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Alcohol</th>\n",
              "      <th>Malicacid</th>\n",
              "      <th>Ash</th>\n",
              "      <th>Alcalinity_of_ash</th>\n",
              "      <th>Magnesium</th>\n",
              "      <th>Total_phenols</th>\n",
              "      <th>Flavanoids</th>\n",
              "      <th>Nonflavanoid_phenols</th>\n",
              "      <th>Proanthocyanins</th>\n",
              "      <th>Color_intensity</th>\n",
              "      <th>Hue</th>\n",
              "      <th>0D280_0D315_of_diluted_wines</th>\n",
              "      <th>Proline</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>14.23</td>\n",
              "      <td>1.71</td>\n",
              "      <td>2.43</td>\n",
              "      <td>15.6</td>\n",
              "      <td>127</td>\n",
              "      <td>2.80</td>\n",
              "      <td>3.06</td>\n",
              "      <td>0.28</td>\n",
              "      <td>2.29</td>\n",
              "      <td>5.64</td>\n",
              "      <td>1.04</td>\n",
              "      <td>3.92</td>\n",
              "      <td>1065</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>13.20</td>\n",
              "      <td>1.78</td>\n",
              "      <td>2.14</td>\n",
              "      <td>11.2</td>\n",
              "      <td>100</td>\n",
              "      <td>2.65</td>\n",
              "      <td>2.76</td>\n",
              "      <td>0.26</td>\n",
              "      <td>1.28</td>\n",
              "      <td>4.38</td>\n",
              "      <td>1.05</td>\n",
              "      <td>3.40</td>\n",
              "      <td>1050</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>13.16</td>\n",
              "      <td>2.36</td>\n",
              "      <td>2.67</td>\n",
              "      <td>18.6</td>\n",
              "      <td>101</td>\n",
              "      <td>2.80</td>\n",
              "      <td>3.24</td>\n",
              "      <td>0.30</td>\n",
              "      <td>2.81</td>\n",
              "      <td>5.68</td>\n",
              "      <td>1.03</td>\n",
              "      <td>3.17</td>\n",
              "      <td>1185</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>14.37</td>\n",
              "      <td>1.95</td>\n",
              "      <td>2.50</td>\n",
              "      <td>16.8</td>\n",
              "      <td>113</td>\n",
              "      <td>3.85</td>\n",
              "      <td>3.49</td>\n",
              "      <td>0.24</td>\n",
              "      <td>2.18</td>\n",
              "      <td>7.80</td>\n",
              "      <td>0.86</td>\n",
              "      <td>3.45</td>\n",
              "      <td>1480</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>13.24</td>\n",
              "      <td>2.59</td>\n",
              "      <td>2.87</td>\n",
              "      <td>21.0</td>\n",
              "      <td>118</td>\n",
              "      <td>2.80</td>\n",
              "      <td>2.69</td>\n",
              "      <td>0.39</td>\n",
              "      <td>1.82</td>\n",
              "      <td>4.32</td>\n",
              "      <td>1.04</td>\n",
              "      <td>2.93</td>\n",
              "      <td>735</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7168aa7c-1364-4e8c-93d6-59ab2b725e8c')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7168aa7c-1364-4e8c-93d6-59ab2b725e8c button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7168aa7c-1364-4e8c-93d6-59ab2b725e8c');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 178,\n  \"fields\": [\n    {\n      \"column\": \"Alcohol\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.8118265380058577,\n        \"min\": 11.03,\n        \"max\": 14.83,\n        \"num_unique_values\": 126,\n        \"samples\": [\n          11.62,\n          13.64,\n          13.69\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Malicacid\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1.1171460976144627,\n        \"min\": 0.74,\n        \"max\": 5.8,\n        \"num_unique_values\": 133,\n        \"samples\": [\n          1.21,\n          2.83,\n          1.8\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Ash\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2743440090608148,\n        \"min\": 1.36,\n        \"max\": 3.23,\n        \"num_unique_values\": 79,\n        \"samples\": [\n          2.31,\n          2.43,\n          2.52\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Alcalinity_of_ash\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.3395637671735052,\n        \"min\": 10.6,\n        \"max\": 30.0,\n        \"num_unique_values\": 63,\n        \"samples\": [\n          25.5,\n          28.5,\n          15.6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Magnesium\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 14,\n        \"min\": 70,\n        \"max\": 162,\n        \"num_unique_values\": 53,\n        \"samples\": [\n          126,\n          85,\n          162\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Total_phenols\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.6258510488339891,\n        \"min\": 0.98,\n        \"max\": 3.88,\n        \"num_unique_values\": 97,\n        \"samples\": [\n          1.68,\n          2.11,\n          1.35\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Flavanoids\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.9988586850169465,\n        \"min\": 0.34,\n        \"max\": 5.08,\n        \"num_unique_values\": 132,\n        \"samples\": [\n          3.18,\n          2.5,\n          3.17\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Nonflavanoid_phenols\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.12445334029667939,\n        \"min\": 0.13,\n        \"max\": 0.66,\n        \"num_unique_values\": 39,\n        \"samples\": [\n          0.58,\n          0.41,\n          0.39\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Proanthocyanins\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.5723588626747611,\n        \"min\": 0.41,\n        \"max\": 3.58,\n        \"num_unique_values\": 101,\n        \"samples\": [\n          0.75,\n          1.77,\n          1.42\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Color_intensity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2.318285871822413,\n        \"min\": 1.28,\n        \"max\": 13.0,\n        \"num_unique_values\": 132,\n        \"samples\": [\n          2.95,\n          3.3,\n          5.1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Hue\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.22857156582982338,\n        \"min\": 0.48,\n        \"max\": 1.71,\n        \"num_unique_values\": 78,\n        \"samples\": [\n          1.22,\n          1.04,\n          1.45\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"0D280_0D315_of_diluted_wines\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.7099904287650505,\n        \"min\": 1.27,\n        \"max\": 4.0,\n        \"num_unique_values\": 122,\n        \"samples\": [\n          4.0,\n          1.82,\n          1.59\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Proline\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 314,\n        \"min\": 278,\n        \"max\": 1680,\n        \"num_unique_values\": 121,\n        \"samples\": [\n          1375,\n          1270,\n          735\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"class\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 3,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          2,\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "id": "OQXyYk4vv3es",
        "outputId": "1ae8d241-6f6f-44cb-96f4-40c0d8734778"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "          Alcohol   Malicacid         Ash  Alcalinity_of_ash   Magnesium  \\\n",
              "count  178.000000  178.000000  178.000000         178.000000  178.000000   \n",
              "mean    13.000618    2.336348    2.366517          19.494944   99.741573   \n",
              "std      0.811827    1.117146    0.274344           3.339564   14.282484   \n",
              "min     11.030000    0.740000    1.360000          10.600000   70.000000   \n",
              "25%     12.362500    1.602500    2.210000          17.200000   88.000000   \n",
              "50%     13.050000    1.865000    2.360000          19.500000   98.000000   \n",
              "75%     13.677500    3.082500    2.557500          21.500000  107.000000   \n",
              "max     14.830000    5.800000    3.230000          30.000000  162.000000   \n",
              "\n",
              "       Total_phenols  Flavanoids  Nonflavanoid_phenols  Proanthocyanins  \\\n",
              "count     178.000000  178.000000            178.000000       178.000000   \n",
              "mean        2.295112    2.029270              0.361854         1.590899   \n",
              "std         0.625851    0.998859              0.124453         0.572359   \n",
              "min         0.980000    0.340000              0.130000         0.410000   \n",
              "25%         1.742500    1.205000              0.270000         1.250000   \n",
              "50%         2.355000    2.135000              0.340000         1.555000   \n",
              "75%         2.800000    2.875000              0.437500         1.950000   \n",
              "max         3.880000    5.080000              0.660000         3.580000   \n",
              "\n",
              "       Color_intensity         Hue  0D280_0D315_of_diluted_wines      Proline  \\\n",
              "count       178.000000  178.000000                    178.000000   178.000000   \n",
              "mean          5.058090    0.957449                      2.611685   746.893258   \n",
              "std           2.318286    0.228572                      0.709990   314.907474   \n",
              "min           1.280000    0.480000                      1.270000   278.000000   \n",
              "25%           3.220000    0.782500                      1.937500   500.500000   \n",
              "50%           4.690000    0.965000                      2.780000   673.500000   \n",
              "75%           6.200000    1.120000                      3.170000   985.000000   \n",
              "max          13.000000    1.710000                      4.000000  1680.000000   \n",
              "\n",
              "            class  \n",
              "count  178.000000  \n",
              "mean     1.938202  \n",
              "std      0.775035  \n",
              "min      1.000000  \n",
              "25%      1.000000  \n",
              "50%      2.000000  \n",
              "75%      3.000000  \n",
              "max      3.000000  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-debd5b00-072f-4da8-a90d-d24d27749dba\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Alcohol</th>\n",
              "      <th>Malicacid</th>\n",
              "      <th>Ash</th>\n",
              "      <th>Alcalinity_of_ash</th>\n",
              "      <th>Magnesium</th>\n",
              "      <th>Total_phenols</th>\n",
              "      <th>Flavanoids</th>\n",
              "      <th>Nonflavanoid_phenols</th>\n",
              "      <th>Proanthocyanins</th>\n",
              "      <th>Color_intensity</th>\n",
              "      <th>Hue</th>\n",
              "      <th>0D280_0D315_of_diluted_wines</th>\n",
              "      <th>Proline</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "      <td>178.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>13.000618</td>\n",
              "      <td>2.336348</td>\n",
              "      <td>2.366517</td>\n",
              "      <td>19.494944</td>\n",
              "      <td>99.741573</td>\n",
              "      <td>2.295112</td>\n",
              "      <td>2.029270</td>\n",
              "      <td>0.361854</td>\n",
              "      <td>1.590899</td>\n",
              "      <td>5.058090</td>\n",
              "      <td>0.957449</td>\n",
              "      <td>2.611685</td>\n",
              "      <td>746.893258</td>\n",
              "      <td>1.938202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.811827</td>\n",
              "      <td>1.117146</td>\n",
              "      <td>0.274344</td>\n",
              "      <td>3.339564</td>\n",
              "      <td>14.282484</td>\n",
              "      <td>0.625851</td>\n",
              "      <td>0.998859</td>\n",
              "      <td>0.124453</td>\n",
              "      <td>0.572359</td>\n",
              "      <td>2.318286</td>\n",
              "      <td>0.228572</td>\n",
              "      <td>0.709990</td>\n",
              "      <td>314.907474</td>\n",
              "      <td>0.775035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>11.030000</td>\n",
              "      <td>0.740000</td>\n",
              "      <td>1.360000</td>\n",
              "      <td>10.600000</td>\n",
              "      <td>70.000000</td>\n",
              "      <td>0.980000</td>\n",
              "      <td>0.340000</td>\n",
              "      <td>0.130000</td>\n",
              "      <td>0.410000</td>\n",
              "      <td>1.280000</td>\n",
              "      <td>0.480000</td>\n",
              "      <td>1.270000</td>\n",
              "      <td>278.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>12.362500</td>\n",
              "      <td>1.602500</td>\n",
              "      <td>2.210000</td>\n",
              "      <td>17.200000</td>\n",
              "      <td>88.000000</td>\n",
              "      <td>1.742500</td>\n",
              "      <td>1.205000</td>\n",
              "      <td>0.270000</td>\n",
              "      <td>1.250000</td>\n",
              "      <td>3.220000</td>\n",
              "      <td>0.782500</td>\n",
              "      <td>1.937500</td>\n",
              "      <td>500.500000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>13.050000</td>\n",
              "      <td>1.865000</td>\n",
              "      <td>2.360000</td>\n",
              "      <td>19.500000</td>\n",
              "      <td>98.000000</td>\n",
              "      <td>2.355000</td>\n",
              "      <td>2.135000</td>\n",
              "      <td>0.340000</td>\n",
              "      <td>1.555000</td>\n",
              "      <td>4.690000</td>\n",
              "      <td>0.965000</td>\n",
              "      <td>2.780000</td>\n",
              "      <td>673.500000</td>\n",
              "      <td>2.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>13.677500</td>\n",
              "      <td>3.082500</td>\n",
              "      <td>2.557500</td>\n",
              "      <td>21.500000</td>\n",
              "      <td>107.000000</td>\n",
              "      <td>2.800000</td>\n",
              "      <td>2.875000</td>\n",
              "      <td>0.437500</td>\n",
              "      <td>1.950000</td>\n",
              "      <td>6.200000</td>\n",
              "      <td>1.120000</td>\n",
              "      <td>3.170000</td>\n",
              "      <td>985.000000</td>\n",
              "      <td>3.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>14.830000</td>\n",
              "      <td>5.800000</td>\n",
              "      <td>3.230000</td>\n",
              "      <td>30.000000</td>\n",
              "      <td>162.000000</td>\n",
              "      <td>3.880000</td>\n",
              "      <td>5.080000</td>\n",
              "      <td>0.660000</td>\n",
              "      <td>3.580000</td>\n",
              "      <td>13.000000</td>\n",
              "      <td>1.710000</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>1680.000000</td>\n",
              "      <td>3.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-debd5b00-072f-4da8-a90d-d24d27749dba')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-debd5b00-072f-4da8-a90d-d24d27749dba button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-debd5b00-072f-4da8-a90d-d24d27749dba');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 8,\n  \"fields\": [\n    {\n      \"column\": \"Alcohol\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 59.11804622765535,\n        \"min\": 0.8118265380058577,\n        \"max\": 178.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          13.00061797752809,\n          13.05,\n          178.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Malicacid\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 62.116878053835904,\n        \"min\": 0.74,\n        \"max\": 178.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          2.3363483146067416,\n          1.8650000000000002,\n          178.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Ash\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 62.2136127716068,\n        \"min\": 0.2743440090608148,\n        \"max\": 178.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          2.3665168539325845,\n          2.36,\n          178.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Alcalinity_of_ash\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 57.32526338306686,\n        \"min\": 3.3395637671735052,\n        \"max\": 178.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          19.49494382022472,\n          19.5,\n          178.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Magnesium\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 51.18081686409121,\n        \"min\": 14.282483515295668,\n        \"max\": 178.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          99.74157303370787,\n          98.0,\n          178.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Total_phenols\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 62.19953212829461,\n        \"min\": 0.6258510488339891,\n        \"max\": 178.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          2.295112359550562,\n          2.355,\n          178.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Flavanoids\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 62.20866774807846,\n        \"min\": 0.34,\n        \"max\": 178.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          2.0292696629213487,\n          2.135,\n          178.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Nonflavanoid_phenols\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 62.81536936528359,\n        \"min\": 0.12445334029667939,\n        \"max\": 178.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.3618539325842696,\n          0.34,\n          178.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Proanthocyanins\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 62.389141510191415,\n        \"min\": 0.41,\n        \"max\": 178.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          1.5908988764044945,\n          1.5550000000000002,\n          178.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Color_intensity\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 61.23070389608627,\n        \"min\": 1.28,\n        \"max\": 178.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          5.058089882022472,\n          4.6899999999999995,\n          178.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Hue\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 62.618701002452724,\n        \"min\": 0.22857156582982338,\n        \"max\": 178.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          0.9574494382022471,\n          0.965,\n          178.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"0D280_0D315_of_diluted_wines\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 62.109005977624996,\n        \"min\": 0.7099904287650505,\n        \"max\": 178.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          2.6116853932584267,\n          2.78,\n          178.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Proline\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 489.2848273029377,\n        \"min\": 178.0,\n        \"max\": 1680.0,\n        \"num_unique_values\": 8,\n        \"samples\": [\n          746.8932584269663,\n          673.5,\n          178.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"class\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 62.2964284117262,\n        \"min\": 0.7750349899850565,\n        \"max\": 178.0,\n        \"num_unique_values\": 6,\n        \"samples\": [\n          178.0,\n          1.9382022471910112,\n          3.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_tensor = torch.tensor(X.values, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y.values, dtype=torch.float32).view(-1, 1)\n",
        "print(\"Shape of X_tensor:\", X_tensor.shape)\n",
        "print(\"Shape of y_tensor:\", y_tensor.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MpU-9kVCwE_1",
        "outputId": "6e7e5b1c-8474-44ff-ae79-afc60d3e9d49"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X_tensor: torch.Size([178, 13])\n",
            "Shape of y_tensor: torch.Size([178, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation (2 marks)"
      ],
      "metadata": {
        "id": "9EfMZYsRrask"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train-Validation-Test Split (1 mark)\n",
        "\n",
        "Split data into:\n",
        "- 70% train\n",
        "- 15% validation\n",
        "- 15% test"
      ],
      "metadata": {
        "id": "j381KAZJrfwM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "dataset = TensorDataset(X_tensor, y_tensor)\n",
        "\n",
        "# Calculate split sizes\n",
        "total_samples = len(dataset)\n",
        "train_size = int(0.7 * total_samples)\n",
        "val_size = int(0.15 * total_samples)\n",
        "test_size = total_samples - train_size - val_size\n",
        "\n",
        "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
        "\n",
        "train_X, train_y = train_dataset[:]\n",
        "val_X, val_y = val_dataset[:]\n",
        "test_X, test_y = test_dataset[:]"
      ],
      "metadata": {
        "id": "j5OXb0J_rbWu"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Normalization (1 mark)\n",
        "\n",
        "Normalize input features only\n",
        "\n",
        "Explain briefly why is normalization important for ANN training?"
      ],
      "metadata": {
        "id": "kVLBLbAVrjN8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "train_X_scaled = torch.tensor(scaler.fit_transform(train_X.numpy()), dtype=torch.float32)\n",
        "val_X_scaled = torch.tensor(scaler.transform(val_X.numpy()), dtype=torch.float32)\n",
        "test_X_scaled = torch.tensor(scaler.transform(test_X.numpy()), dtype=torch.float32)\n",
        "print(\"Shape of train_X_scaled:\", train_X_scaled.shape)\n",
        "print(\"Shape of train_y:\", train_y.shape)\n",
        "print(\"Shape of val_X_scaled:\", val_X_scaled.shape)\n",
        "print(\"Shape of val_y:\", val_y.shape)\n",
        "print(\"Shape of test_X_scaled:\", test_X_scaled.shape)\n",
        "print(\"Shape of test_y:\", test_y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GmlSZB5CrkPu",
        "outputId": "7a3579c4-ff41-4954-d477-2fc6ee099f50"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of train_X_scaled: torch.Size([124, 13])\n",
            "Shape of train_y: torch.Size([124, 1])\n",
            "Shape of val_X_scaled: torch.Size([26, 13])\n",
            "Shape of val_y: torch.Size([26, 1])\n",
            "Shape of test_X_scaled: torch.Size([28, 13])\n",
            "Shape of test_y: torch.Size([28, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Normalization is crucial for ANN training because it helps stabilize and speed up the learning process by ensuring that all features contribute equally to the model. It prevents features with larger scales from dominating the learning and helps avoid issues like exploding or vanishing gradients."
      ],
      "metadata": {
        "id": "PAy-vRMpwlxF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ANN Model Design (4 marks)"
      ],
      "metadata": {
        "id": "c7hDHvHXroJE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Architecture (3 marks)\n",
        "\n",
        "Implement the following ANNs:\n",
        "\n",
        "1. Architecture 1\n",
        "\n",
        "    Hidden Layer 1 → 64 neurons, ReLU\n",
        "\n",
        "    Hidden Layer 2 → 32 neurons, ReLU\n",
        "\n",
        "    Output Layer → 1 neuron, Linear\n",
        "\n",
        "2. Architecture 2\n",
        "\n",
        "    Hidden Layer 1 → 64 neurons, Tanh\n",
        "\n",
        "    Hidden Layer 2 → 32 neurons, Tanh\n",
        "\n",
        "    Output Layer → 1 neuron, Linear\n",
        "\n",
        "3. Architecture 3\n",
        "\n",
        "    Hidden Layer 1 → 128 neurons, ReLU\n",
        "\n",
        "    Hidden Layer 2 → 64 neurons, ReLU\n",
        "\n",
        "    Hidden Layer 3 → 32 neurons, ReLU\n",
        "\n",
        "    Output Layer → 1 neuron, Linear\n",
        "\n",
        "4. Architecture 4\n",
        "\n",
        "    Hidden Layer 1 → 512 neurons, ReLU\n",
        "\n",
        "    Hidden Layer 2 → 256 neurons, LeakyReLU\n",
        "\n",
        "    Hidden Layer 3 → 128 neurons, Tanh\n",
        "\n",
        "    Hidden Layer 4 → 64 neurons, Sigmoid\n",
        "\n",
        "    Output Layer → 1 neuron, Linear\n",
        "\n",
        "Batch sizes to be used 16, 32 and 64 for each architecture."
      ],
      "metadata": {
        "id": "NZS3wPJprtUV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WineQualityANN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_layers, activations):\n",
        "        super(WineQualityANN, self).__init__()\n",
        "        layers = []\n",
        "        in_features = input_size\n",
        "        for i, (out_features, activation) in enumerate(zip(hidden_layers, activations)):\n",
        "            layers.append(nn.Linear(in_features, out_features))\n",
        "            if activation == 'ReLU':\n",
        "                layers.append(nn.ReLU())\n",
        "            elif activation == 'Tanh':\n",
        "                layers.append(nn.Tanh())\n",
        "            elif activation == 'LeakyReLU':\n",
        "                layers.append(nn.LeakyReLU())\n",
        "            elif activation == 'Sigmoid':\n",
        "                layers.append(nn.Sigmoid())\n",
        "            elif activation == 'Linear':\n",
        "                pass\n",
        "            else:\n",
        "                raise ValueError(f\"Unsupported activation function: {activation}\")\n",
        "            in_features = out_features\n",
        "        layers.append(nn.Linear(in_features, 1))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Architecture 1\n",
        "class Architecture1(WineQualityANN):\n",
        "    def __init__(self, input_size):\n",
        "        super().__init__(input_size, [64, 32], ['ReLU', 'ReLU'])\n",
        "\n",
        "# Architecture 2\n",
        "class Architecture2(WineQualityANN):\n",
        "    def __init__(self, input_size):\n",
        "        super().__init__(input_size, [64, 32], ['Tanh', 'Tanh'])\n",
        "\n",
        "# Architecture 3\n",
        "class Architecture3(WineQualityANN):\n",
        "    def __init__(self, input_size):\n",
        "        super().__init__(input_size, [128, 64, 32], ['ReLU', 'ReLU', 'ReLU'])\n",
        "\n",
        "# Architecture 4\n",
        "class Architecture4(WineQualityANN):\n",
        "    def __init__(self, input_size):\n",
        "        super().__init__(input_size, [512, 256, 128, 64], ['ReLU', 'LeakyReLU', 'Tanh', 'Sigmoid'])\n",
        "\n",
        "input_size = train_X_scaled.shape[1]\n",
        "model1 = Architecture1(input_size)\n",
        "model2 = Architecture2(input_size)\n",
        "model3 = Architecture3(input_size)\n",
        "model4 = Architecture4(input_size)\n",
        "\n",
        "print(\"Architecture 1 instantiated successfully:\", model1)\n",
        "print(\"Architecture 2 instantiated successfully:\", model2)\n",
        "print(\"Architecture 3 instantiated successfully:\", model3)\n",
        "print(\"Architecture 4 instantiated successfully:\", model4)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e7Ja-N8Qrorm",
        "outputId": "6208f20c-9ae7-4dac-c58f-9c089e54a6d6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Architecture 1 instantiated successfully: Architecture1(\n",
            "  (model): Sequential(\n",
            "    (0): Linear(in_features=13, out_features=64, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=32, out_features=1, bias=True)\n",
            "  )\n",
            ")\n",
            "Architecture 2 instantiated successfully: Architecture2(\n",
            "  (model): Sequential(\n",
            "    (0): Linear(in_features=13, out_features=64, bias=True)\n",
            "    (1): Tanh()\n",
            "    (2): Linear(in_features=64, out_features=32, bias=True)\n",
            "    (3): Tanh()\n",
            "    (4): Linear(in_features=32, out_features=1, bias=True)\n",
            "  )\n",
            ")\n",
            "Architecture 3 instantiated successfully: Architecture3(\n",
            "  (model): Sequential(\n",
            "    (0): Linear(in_features=13, out_features=128, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=128, out_features=64, bias=True)\n",
            "    (3): ReLU()\n",
            "    (4): Linear(in_features=64, out_features=32, bias=True)\n",
            "    (5): ReLU()\n",
            "    (6): Linear(in_features=32, out_features=1, bias=True)\n",
            "  )\n",
            ")\n",
            "Architecture 4 instantiated successfully: Architecture4(\n",
            "  (model): Sequential(\n",
            "    (0): Linear(in_features=13, out_features=512, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Linear(in_features=512, out_features=256, bias=True)\n",
            "    (3): LeakyReLU(negative_slope=0.01)\n",
            "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
            "    (5): Tanh()\n",
            "    (6): Linear(in_features=128, out_features=64, bias=True)\n",
            "    (7): Sigmoid()\n",
            "    (8): Linear(in_features=64, out_features=1, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why should the output layer not use ReLU or Softmax for this task? (1 mark)"
      ],
      "metadata": {
        "id": "jWd62v1Xrx-M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ReLU (Rectified Linear Unit) is unsuitable because it outputs 0 for any negative input, which would cap the model's ability to predict negative values if they were relevant.\n",
        "\n",
        "Softmax is unsuitable because it outputs a probability distribution over multiple classes, meaning the sum of its outputs is always 1. This is not appropriate for predicting a single, continuous regression value."
      ],
      "metadata": {
        "id": "GJbPj-iBxZZM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Model (8 marks)"
      ],
      "metadata": {
        "id": "46IaYcUfr32U"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Setup (4 marks)\n",
        "\n",
        "Choose:\n",
        "1. Loss function\n",
        "2. Optimizer\n",
        "\n",
        "Justify both choices in 1 sentence each"
      ],
      "metadata": {
        "id": "n_AdAYiYr85k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For this regression problem, `Mean Squared Error (MSE)` is chosen as the loss function because it directly measures the average squared difference between the predicted and actual values, which is suitable for quantifying the error in continuous predictions. `Adam` is chosen as the optimizer because it is an adaptive learning rate optimization algorithm that is computationally efficient, has little memory requirement, and is well-suited for problems with large datasets and many parameters, often converging faster and performing better than other optimizers."
      ],
      "metadata": {
        "id": "ZtlAxm6jx4Bc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training Loop (4 marks)\n",
        "\n",
        "Train the model for 50 epochs\n",
        "\n",
        "Print:\n",
        "1. Training loss\n",
        "2. Validation loss (every 5 epochs)"
      ],
      "metadata": {
        "id": "tV8gjqP2sBjs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.MSELoss()\n",
        "\n",
        "architectures = {\n",
        "    \"Architecture 1\": Architecture1(input_size),\n",
        "    \"Architecture 2\": Architecture2(input_size),\n",
        "    \"Architecture 3\": Architecture3(input_size),\n",
        "    \"Architecture 4\": Architecture4(input_size)\n",
        "}\n",
        "\n",
        "batch_sizes = [16, 32, 64]\n",
        "num_epochs = 50\n",
        "\n",
        "results = {}\n",
        "\n",
        "for arch_name, base_model in architectures.items():\n",
        "    results[arch_name] = {}\n",
        "    for batch_size in batch_sizes:\n",
        "        print(f\"\\nTraining {arch_name} with Batch Size: {batch_size}\")\n",
        "        train_dataset_tensor = TensorDataset(train_X_scaled, train_y)\n",
        "        val_dataset_tensor = TensorDataset(val_X_scaled, val_y)\n",
        "        train_loader = DataLoader(train_dataset_tensor, batch_size=batch_size, shuffle=True)\n",
        "        val_loader = DataLoader(val_dataset_tensor, batch_size=batch_size, shuffle=False)\n",
        "        model = type(base_model)(input_size)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "        history = {'train_loss': [], 'val_loss': []}\n",
        "        for epoch in range(num_epochs):\n",
        "            model.train()\n",
        "            running_train_loss = 0.0\n",
        "            for inputs, targets in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                running_train_loss += loss.item() * inputs.size(0)\n",
        "            epoch_train_loss = running_train_loss / len(train_loader.dataset)\n",
        "            history['train_loss'].append(epoch_train_loss)\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {epoch_train_loss:.4f}\", end=\"\")\n",
        "            if (epoch + 1) % 5 == 0:\n",
        "                model.eval()\n",
        "                running_val_loss = 0.0\n",
        "                with torch.no_grad():\n",
        "                    for inputs, targets in val_loader:\n",
        "                        outputs = model(inputs)\n",
        "                        loss = criterion(outputs, targets)\n",
        "                        running_val_loss += loss.item() * inputs.size(0)\n",
        "                epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
        "                history['val_loss'].append(epoch_val_loss)\n",
        "                print(f\", Val Loss: {epoch_val_loss:.4f}\")\n",
        "            else:\n",
        "                print(\"\")\n",
        "        results[arch_name][batch_size] = {'model': model, 'history': history}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LTky4LEGsDOF",
        "outputId": "bd2cf008-9bba-4acd-e9e4-e8924a9719fb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Training Architecture 1 with Batch Size: 16\n",
            "Epoch 1/50, Train Loss: 3.5722\n",
            "Epoch 2/50, Train Loss: 2.6150\n",
            "Epoch 3/50, Train Loss: 1.7322\n",
            "Epoch 4/50, Train Loss: 1.0143\n",
            "Epoch 5/50, Train Loss: 0.4492, Val Loss: 0.3438\n",
            "Epoch 6/50, Train Loss: 0.2586\n",
            "Epoch 7/50, Train Loss: 0.2082\n",
            "Epoch 8/50, Train Loss: 0.1671\n",
            "Epoch 9/50, Train Loss: 0.1358\n",
            "Epoch 10/50, Train Loss: 0.1246, Val Loss: 0.1229\n",
            "Epoch 11/50, Train Loss: 0.1137\n",
            "Epoch 12/50, Train Loss: 0.1028\n",
            "Epoch 13/50, Train Loss: 0.0966\n",
            "Epoch 14/50, Train Loss: 0.0904\n",
            "Epoch 15/50, Train Loss: 0.0849, Val Loss: 0.0706\n",
            "Epoch 16/50, Train Loss: 0.0801\n",
            "Epoch 17/50, Train Loss: 0.0755\n",
            "Epoch 18/50, Train Loss: 0.0721\n",
            "Epoch 19/50, Train Loss: 0.0692\n",
            "Epoch 20/50, Train Loss: 0.0658, Val Loss: 0.0593\n",
            "Epoch 21/50, Train Loss: 0.0635\n",
            "Epoch 22/50, Train Loss: 0.0608\n",
            "Epoch 23/50, Train Loss: 0.0592\n",
            "Epoch 24/50, Train Loss: 0.0567\n",
            "Epoch 25/50, Train Loss: 0.0547, Val Loss: 0.0520\n",
            "Epoch 26/50, Train Loss: 0.0543\n",
            "Epoch 27/50, Train Loss: 0.0518\n",
            "Epoch 28/50, Train Loss: 0.0504\n",
            "Epoch 29/50, Train Loss: 0.0492\n",
            "Epoch 30/50, Train Loss: 0.0476, Val Loss: 0.0491\n",
            "Epoch 31/50, Train Loss: 0.0460\n",
            "Epoch 32/50, Train Loss: 0.0449\n",
            "Epoch 33/50, Train Loss: 0.0436\n",
            "Epoch 34/50, Train Loss: 0.0434\n",
            "Epoch 35/50, Train Loss: 0.0421, Val Loss: 0.0494\n",
            "Epoch 36/50, Train Loss: 0.0407\n",
            "Epoch 37/50, Train Loss: 0.0399\n",
            "Epoch 38/50, Train Loss: 0.0385\n",
            "Epoch 39/50, Train Loss: 0.0380\n",
            "Epoch 40/50, Train Loss: 0.0370, Val Loss: 0.0447\n",
            "Epoch 41/50, Train Loss: 0.0362\n",
            "Epoch 42/50, Train Loss: 0.0363\n",
            "Epoch 43/50, Train Loss: 0.0354\n",
            "Epoch 44/50, Train Loss: 0.0341\n",
            "Epoch 45/50, Train Loss: 0.0335, Val Loss: 0.0404\n",
            "Epoch 46/50, Train Loss: 0.0331\n",
            "Epoch 47/50, Train Loss: 0.0321\n",
            "Epoch 48/50, Train Loss: 0.0311\n",
            "Epoch 49/50, Train Loss: 0.0307\n",
            "Epoch 50/50, Train Loss: 0.0300, Val Loss: 0.0412\n",
            "\n",
            "Training Architecture 1 with Batch Size: 32\n",
            "Epoch 1/50, Train Loss: 4.5126\n",
            "Epoch 2/50, Train Loss: 3.9639\n",
            "Epoch 3/50, Train Loss: 3.4227\n",
            "Epoch 4/50, Train Loss: 2.9047\n",
            "Epoch 5/50, Train Loss: 2.4055, Val Loss: 2.4130\n",
            "Epoch 6/50, Train Loss: 1.9085\n",
            "Epoch 7/50, Train Loss: 1.4089\n",
            "Epoch 8/50, Train Loss: 1.0009\n",
            "Epoch 9/50, Train Loss: 0.6486\n",
            "Epoch 10/50, Train Loss: 0.4245, Val Loss: 0.4331\n",
            "Epoch 11/50, Train Loss: 0.2750\n",
            "Epoch 12/50, Train Loss: 0.2164\n",
            "Epoch 13/50, Train Loss: 0.2117\n",
            "Epoch 14/50, Train Loss: 0.2055\n",
            "Epoch 15/50, Train Loss: 0.1841, Val Loss: 0.1166\n",
            "Epoch 16/50, Train Loss: 0.1566\n",
            "Epoch 17/50, Train Loss: 0.1365\n",
            "Epoch 18/50, Train Loss: 0.1283\n",
            "Epoch 19/50, Train Loss: 0.1203\n",
            "Epoch 20/50, Train Loss: 0.1166, Val Loss: 0.1559\n",
            "Epoch 21/50, Train Loss: 0.1124\n",
            "Epoch 22/50, Train Loss: 0.1065\n",
            "Epoch 23/50, Train Loss: 0.1009\n",
            "Epoch 24/50, Train Loss: 0.0974\n",
            "Epoch 25/50, Train Loss: 0.0945, Val Loss: 0.1052\n",
            "Epoch 26/50, Train Loss: 0.0916\n",
            "Epoch 27/50, Train Loss: 0.0890\n",
            "Epoch 28/50, Train Loss: 0.0863\n",
            "Epoch 29/50, Train Loss: 0.0840\n",
            "Epoch 30/50, Train Loss: 0.0815, Val Loss: 0.0946\n",
            "Epoch 31/50, Train Loss: 0.0794\n",
            "Epoch 32/50, Train Loss: 0.0772\n",
            "Epoch 33/50, Train Loss: 0.0753\n",
            "Epoch 34/50, Train Loss: 0.0731\n",
            "Epoch 35/50, Train Loss: 0.0717, Val Loss: 0.0845\n",
            "Epoch 36/50, Train Loss: 0.0701\n",
            "Epoch 37/50, Train Loss: 0.0684\n",
            "Epoch 38/50, Train Loss: 0.0666\n",
            "Epoch 39/50, Train Loss: 0.0650\n",
            "Epoch 40/50, Train Loss: 0.0636, Val Loss: 0.0789\n",
            "Epoch 41/50, Train Loss: 0.0622\n",
            "Epoch 42/50, Train Loss: 0.0608\n",
            "Epoch 43/50, Train Loss: 0.0598\n",
            "Epoch 44/50, Train Loss: 0.0587\n",
            "Epoch 45/50, Train Loss: 0.0574, Val Loss: 0.0748\n",
            "Epoch 46/50, Train Loss: 0.0562\n",
            "Epoch 47/50, Train Loss: 0.0553\n",
            "Epoch 48/50, Train Loss: 0.0542\n",
            "Epoch 49/50, Train Loss: 0.0533\n",
            "Epoch 50/50, Train Loss: 0.0523, Val Loss: 0.0710\n",
            "\n",
            "Training Architecture 1 with Batch Size: 64\n",
            "Epoch 1/50, Train Loss: 3.3817\n",
            "Epoch 2/50, Train Loss: 3.1507\n",
            "Epoch 3/50, Train Loss: 2.9126\n",
            "Epoch 4/50, Train Loss: 2.6694\n",
            "Epoch 5/50, Train Loss: 2.4398, Val Loss: 2.4003\n",
            "Epoch 6/50, Train Loss: 2.2052\n",
            "Epoch 7/50, Train Loss: 1.9690\n",
            "Epoch 8/50, Train Loss: 1.7408\n",
            "Epoch 9/50, Train Loss: 1.5139\n",
            "Epoch 10/50, Train Loss: 1.2933, Val Loss: 1.2841\n",
            "Epoch 11/50, Train Loss: 1.0869\n",
            "Epoch 12/50, Train Loss: 0.8923\n",
            "Epoch 13/50, Train Loss: 0.7201\n",
            "Epoch 14/50, Train Loss: 0.5564\n",
            "Epoch 15/50, Train Loss: 0.4336, Val Loss: 0.4556\n",
            "Epoch 16/50, Train Loss: 0.3204\n",
            "Epoch 17/50, Train Loss: 0.2455\n",
            "Epoch 18/50, Train Loss: 0.1930\n",
            "Epoch 19/50, Train Loss: 0.1671\n",
            "Epoch 20/50, Train Loss: 0.1570, Val Loss: 0.1536\n",
            "Epoch 21/50, Train Loss: 0.1611\n",
            "Epoch 22/50, Train Loss: 0.1679\n",
            "Epoch 23/50, Train Loss: 0.1694\n",
            "Epoch 24/50, Train Loss: 0.1660\n",
            "Epoch 25/50, Train Loss: 0.1557, Val Loss: 0.0937\n",
            "Epoch 26/50, Train Loss: 0.1416\n",
            "Epoch 27/50, Train Loss: 0.1281\n",
            "Epoch 28/50, Train Loss: 0.1149\n",
            "Epoch 29/50, Train Loss: 0.1049\n",
            "Epoch 30/50, Train Loss: 0.0984, Val Loss: 0.1006\n",
            "Epoch 31/50, Train Loss: 0.0946\n",
            "Epoch 32/50, Train Loss: 0.0928\n",
            "Epoch 33/50, Train Loss: 0.0922\n",
            "Epoch 34/50, Train Loss: 0.0914\n",
            "Epoch 35/50, Train Loss: 0.0899, Val Loss: 0.1186\n",
            "Epoch 36/50, Train Loss: 0.0880\n",
            "Epoch 37/50, Train Loss: 0.0857\n",
            "Epoch 38/50, Train Loss: 0.0832\n",
            "Epoch 39/50, Train Loss: 0.0803\n",
            "Epoch 40/50, Train Loss: 0.0782, Val Loss: 0.0953\n",
            "Epoch 41/50, Train Loss: 0.0761\n",
            "Epoch 42/50, Train Loss: 0.0749\n",
            "Epoch 43/50, Train Loss: 0.0735\n",
            "Epoch 44/50, Train Loss: 0.0723\n",
            "Epoch 45/50, Train Loss: 0.0712, Val Loss: 0.0811\n",
            "Epoch 46/50, Train Loss: 0.0703\n",
            "Epoch 47/50, Train Loss: 0.0694\n",
            "Epoch 48/50, Train Loss: 0.0682\n",
            "Epoch 49/50, Train Loss: 0.0674\n",
            "Epoch 50/50, Train Loss: 0.0663, Val Loss: 0.0798\n",
            "\n",
            "Training Architecture 2 with Batch Size: 16\n",
            "Epoch 1/50, Train Loss: 4.4477\n",
            "Epoch 2/50, Train Loss: 3.7075\n",
            "Epoch 3/50, Train Loss: 3.0046\n",
            "Epoch 4/50, Train Loss: 2.3243\n",
            "Epoch 5/50, Train Loss: 1.7054, Val Loss: 1.6575\n",
            "Epoch 6/50, Train Loss: 1.1313\n",
            "Epoch 7/50, Train Loss: 0.6633\n",
            "Epoch 8/50, Train Loss: 0.3345\n",
            "Epoch 9/50, Train Loss: 0.1651\n",
            "Epoch 10/50, Train Loss: 0.1087, Val Loss: 0.1224\n",
            "Epoch 11/50, Train Loss: 0.0976\n",
            "Epoch 12/50, Train Loss: 0.0911\n",
            "Epoch 13/50, Train Loss: 0.0843\n",
            "Epoch 14/50, Train Loss: 0.0780\n",
            "Epoch 15/50, Train Loss: 0.0744, Val Loss: 0.1137\n",
            "Epoch 16/50, Train Loss: 0.0708\n",
            "Epoch 17/50, Train Loss: 0.0695\n",
            "Epoch 18/50, Train Loss: 0.0673\n",
            "Epoch 19/50, Train Loss: 0.0650\n",
            "Epoch 20/50, Train Loss: 0.0638, Val Loss: 0.1153\n",
            "Epoch 21/50, Train Loss: 0.0621\n",
            "Epoch 22/50, Train Loss: 0.0612\n",
            "Epoch 23/50, Train Loss: 0.0592\n",
            "Epoch 24/50, Train Loss: 0.0590\n",
            "Epoch 25/50, Train Loss: 0.0574, Val Loss: 0.1143\n",
            "Epoch 26/50, Train Loss: 0.0567\n",
            "Epoch 27/50, Train Loss: 0.0552\n",
            "Epoch 28/50, Train Loss: 0.0544\n",
            "Epoch 29/50, Train Loss: 0.0538\n",
            "Epoch 30/50, Train Loss: 0.0529, Val Loss: 0.1131\n",
            "Epoch 31/50, Train Loss: 0.0520\n",
            "Epoch 32/50, Train Loss: 0.0511\n",
            "Epoch 33/50, Train Loss: 0.0509\n",
            "Epoch 34/50, Train Loss: 0.0502\n",
            "Epoch 35/50, Train Loss: 0.0488, Val Loss: 0.1101\n",
            "Epoch 36/50, Train Loss: 0.0476\n",
            "Epoch 37/50, Train Loss: 0.0475\n",
            "Epoch 38/50, Train Loss: 0.0462\n",
            "Epoch 39/50, Train Loss: 0.0461\n",
            "Epoch 40/50, Train Loss: 0.0449, Val Loss: 0.1094\n",
            "Epoch 41/50, Train Loss: 0.0445\n",
            "Epoch 42/50, Train Loss: 0.0438\n",
            "Epoch 43/50, Train Loss: 0.0428\n",
            "Epoch 44/50, Train Loss: 0.0423\n",
            "Epoch 45/50, Train Loss: 0.0413, Val Loss: 0.1089\n",
            "Epoch 46/50, Train Loss: 0.0408\n",
            "Epoch 47/50, Train Loss: 0.0401\n",
            "Epoch 48/50, Train Loss: 0.0395\n",
            "Epoch 49/50, Train Loss: 0.0388\n",
            "Epoch 50/50, Train Loss: 0.0377, Val Loss: 0.1051\n",
            "\n",
            "Training Architecture 2 with Batch Size: 32\n",
            "Epoch 1/50, Train Loss: 3.8264\n",
            "Epoch 2/50, Train Loss: 3.2913\n",
            "Epoch 3/50, Train Loss: 2.8311\n",
            "Epoch 4/50, Train Loss: 2.4774\n",
            "Epoch 5/50, Train Loss: 2.1546, Val Loss: 2.4952\n",
            "Epoch 6/50, Train Loss: 1.8585\n",
            "Epoch 7/50, Train Loss: 1.5903\n",
            "Epoch 8/50, Train Loss: 1.3324\n",
            "Epoch 9/50, Train Loss: 1.0762\n",
            "Epoch 10/50, Train Loss: 0.8495, Val Loss: 1.0418\n",
            "Epoch 11/50, Train Loss: 0.6534\n",
            "Epoch 12/50, Train Loss: 0.4819\n",
            "Epoch 13/50, Train Loss: 0.3458\n",
            "Epoch 14/50, Train Loss: 0.2447\n",
            "Epoch 15/50, Train Loss: 0.1760, Val Loss: 0.2472\n",
            "Epoch 16/50, Train Loss: 0.1329\n",
            "Epoch 17/50, Train Loss: 0.1109\n",
            "Epoch 18/50, Train Loss: 0.1009\n",
            "Epoch 19/50, Train Loss: 0.0963\n",
            "Epoch 20/50, Train Loss: 0.0940, Val Loss: 0.1145\n",
            "Epoch 21/50, Train Loss: 0.0901\n",
            "Epoch 22/50, Train Loss: 0.0849\n",
            "Epoch 23/50, Train Loss: 0.0805\n",
            "Epoch 24/50, Train Loss: 0.0759\n",
            "Epoch 25/50, Train Loss: 0.0722, Val Loss: 0.1009\n",
            "Epoch 26/50, Train Loss: 0.0700\n",
            "Epoch 27/50, Train Loss: 0.0677\n",
            "Epoch 28/50, Train Loss: 0.0662\n",
            "Epoch 29/50, Train Loss: 0.0645\n",
            "Epoch 30/50, Train Loss: 0.0632, Val Loss: 0.1072\n",
            "Epoch 31/50, Train Loss: 0.0616\n",
            "Epoch 32/50, Train Loss: 0.0601\n",
            "Epoch 33/50, Train Loss: 0.0590\n",
            "Epoch 34/50, Train Loss: 0.0577\n",
            "Epoch 35/50, Train Loss: 0.0569, Val Loss: 0.0981\n",
            "Epoch 36/50, Train Loss: 0.0555\n",
            "Epoch 37/50, Train Loss: 0.0544\n",
            "Epoch 38/50, Train Loss: 0.0534\n",
            "Epoch 39/50, Train Loss: 0.0525\n",
            "Epoch 40/50, Train Loss: 0.0518, Val Loss: 0.0943\n",
            "Epoch 41/50, Train Loss: 0.0508\n",
            "Epoch 42/50, Train Loss: 0.0500\n",
            "Epoch 43/50, Train Loss: 0.0492\n",
            "Epoch 44/50, Train Loss: 0.0485\n",
            "Epoch 45/50, Train Loss: 0.0477, Val Loss: 0.0926\n",
            "Epoch 46/50, Train Loss: 0.0471\n",
            "Epoch 47/50, Train Loss: 0.0466\n",
            "Epoch 48/50, Train Loss: 0.0455\n",
            "Epoch 49/50, Train Loss: 0.0448\n",
            "Epoch 50/50, Train Loss: 0.0440, Val Loss: 0.0887\n",
            "\n",
            "Training Architecture 2 with Batch Size: 64\n",
            "Epoch 1/50, Train Loss: 5.1322\n",
            "Epoch 2/50, Train Loss: 4.8860\n",
            "Epoch 3/50, Train Loss: 4.6492\n",
            "Epoch 4/50, Train Loss: 4.4285\n",
            "Epoch 5/50, Train Loss: 4.1963, Val Loss: 4.5523\n",
            "Epoch 6/50, Train Loss: 3.9757\n",
            "Epoch 7/50, Train Loss: 3.7657\n",
            "Epoch 8/50, Train Loss: 3.5628\n",
            "Epoch 9/50, Train Loss: 3.3457\n",
            "Epoch 10/50, Train Loss: 3.1373, Val Loss: 3.5994\n",
            "Epoch 11/50, Train Loss: 2.9340\n",
            "Epoch 12/50, Train Loss: 2.7286\n",
            "Epoch 13/50, Train Loss: 2.5298\n",
            "Epoch 14/50, Train Loss: 2.3280\n",
            "Epoch 15/50, Train Loss: 2.1289, Val Loss: 2.5931\n",
            "Epoch 16/50, Train Loss: 1.9302\n",
            "Epoch 17/50, Train Loss: 1.7328\n",
            "Epoch 18/50, Train Loss: 1.5454\n",
            "Epoch 19/50, Train Loss: 1.3575\n",
            "Epoch 20/50, Train Loss: 1.1797, Val Loss: 1.5301\n",
            "Epoch 21/50, Train Loss: 1.0106\n",
            "Epoch 22/50, Train Loss: 0.8538\n",
            "Epoch 23/50, Train Loss: 0.7122\n",
            "Epoch 24/50, Train Loss: 0.5811\n",
            "Epoch 25/50, Train Loss: 0.4690, Val Loss: 0.6582\n",
            "Epoch 26/50, Train Loss: 0.3697\n",
            "Epoch 27/50, Train Loss: 0.2895\n",
            "Epoch 28/50, Train Loss: 0.2255\n",
            "Epoch 29/50, Train Loss: 0.1728\n",
            "Epoch 30/50, Train Loss: 0.1340, Val Loss: 0.2235\n",
            "Epoch 31/50, Train Loss: 0.1099\n",
            "Epoch 32/50, Train Loss: 0.0937\n",
            "Epoch 33/50, Train Loss: 0.0853\n",
            "Epoch 34/50, Train Loss: 0.0802\n",
            "Epoch 35/50, Train Loss: 0.0796, Val Loss: 0.1133\n",
            "Epoch 36/50, Train Loss: 0.0805\n",
            "Epoch 37/50, Train Loss: 0.0810\n",
            "Epoch 38/50, Train Loss: 0.0816\n",
            "Epoch 39/50, Train Loss: 0.0813\n",
            "Epoch 40/50, Train Loss: 0.0801, Val Loss: 0.1041\n",
            "Epoch 41/50, Train Loss: 0.0781\n",
            "Epoch 42/50, Train Loss: 0.0759\n",
            "Epoch 43/50, Train Loss: 0.0732\n",
            "Epoch 44/50, Train Loss: 0.0704\n",
            "Epoch 45/50, Train Loss: 0.0679, Val Loss: 0.1119\n",
            "Epoch 46/50, Train Loss: 0.0656\n",
            "Epoch 47/50, Train Loss: 0.0635\n",
            "Epoch 48/50, Train Loss: 0.0620\n",
            "Epoch 49/50, Train Loss: 0.0607\n",
            "Epoch 50/50, Train Loss: 0.0596, Val Loss: 0.1220\n",
            "\n",
            "Training Architecture 3 with Batch Size: 16\n",
            "Epoch 1/50, Train Loss: 3.8413\n",
            "Epoch 2/50, Train Loss: 2.8589\n",
            "Epoch 3/50, Train Loss: 1.4469\n",
            "Epoch 4/50, Train Loss: 0.3917\n",
            "Epoch 5/50, Train Loss: 0.3147, Val Loss: 0.0735\n",
            "Epoch 6/50, Train Loss: 0.1371\n",
            "Epoch 7/50, Train Loss: 0.1378\n",
            "Epoch 8/50, Train Loss: 0.1053\n",
            "Epoch 9/50, Train Loss: 0.0918\n",
            "Epoch 10/50, Train Loss: 0.0834, Val Loss: 0.0897\n",
            "Epoch 11/50, Train Loss: 0.0765\n",
            "Epoch 12/50, Train Loss: 0.0711\n",
            "Epoch 13/50, Train Loss: 0.0692\n",
            "Epoch 14/50, Train Loss: 0.0645\n",
            "Epoch 15/50, Train Loss: 0.0591, Val Loss: 0.0720\n",
            "Epoch 16/50, Train Loss: 0.0569\n",
            "Epoch 17/50, Train Loss: 0.0533\n",
            "Epoch 18/50, Train Loss: 0.0523\n",
            "Epoch 19/50, Train Loss: 0.0503\n",
            "Epoch 20/50, Train Loss: 0.0475, Val Loss: 0.0661\n",
            "Epoch 21/50, Train Loss: 0.0457\n",
            "Epoch 22/50, Train Loss: 0.0439\n",
            "Epoch 23/50, Train Loss: 0.0423\n",
            "Epoch 24/50, Train Loss: 0.0402\n",
            "Epoch 25/50, Train Loss: 0.0402, Val Loss: 0.0605\n",
            "Epoch 26/50, Train Loss: 0.0386\n",
            "Epoch 27/50, Train Loss: 0.0360\n",
            "Epoch 28/50, Train Loss: 0.0360\n",
            "Epoch 29/50, Train Loss: 0.0335\n",
            "Epoch 30/50, Train Loss: 0.0327, Val Loss: 0.0578\n",
            "Epoch 31/50, Train Loss: 0.0316\n",
            "Epoch 32/50, Train Loss: 0.0310\n",
            "Epoch 33/50, Train Loss: 0.0294\n",
            "Epoch 34/50, Train Loss: 0.0283\n",
            "Epoch 35/50, Train Loss: 0.0274, Val Loss: 0.0554\n",
            "Epoch 36/50, Train Loss: 0.0276\n",
            "Epoch 37/50, Train Loss: 0.0257\n",
            "Epoch 38/50, Train Loss: 0.0244\n",
            "Epoch 39/50, Train Loss: 0.0244\n",
            "Epoch 40/50, Train Loss: 0.0237, Val Loss: 0.0530\n",
            "Epoch 41/50, Train Loss: 0.0242\n",
            "Epoch 42/50, Train Loss: 0.0216\n",
            "Epoch 43/50, Train Loss: 0.0218\n",
            "Epoch 44/50, Train Loss: 0.0205\n",
            "Epoch 45/50, Train Loss: 0.0194, Val Loss: 0.0514\n",
            "Epoch 46/50, Train Loss: 0.0175\n",
            "Epoch 47/50, Train Loss: 0.0172\n",
            "Epoch 48/50, Train Loss: 0.0170\n",
            "Epoch 49/50, Train Loss: 0.0153\n",
            "Epoch 50/50, Train Loss: 0.0149, Val Loss: 0.0482\n",
            "\n",
            "Training Architecture 3 with Batch Size: 32\n",
            "Epoch 1/50, Train Loss: 4.1835\n",
            "Epoch 2/50, Train Loss: 3.6693\n",
            "Epoch 3/50, Train Loss: 3.0633\n",
            "Epoch 4/50, Train Loss: 2.3180\n",
            "Epoch 5/50, Train Loss: 1.4848, Val Loss: 1.2066\n",
            "Epoch 6/50, Train Loss: 0.7558\n",
            "Epoch 7/50, Train Loss: 0.3445\n",
            "Epoch 8/50, Train Loss: 0.3274\n",
            "Epoch 9/50, Train Loss: 0.3084\n",
            "Epoch 10/50, Train Loss: 0.1830, Val Loss: 0.0734\n",
            "Epoch 11/50, Train Loss: 0.1263\n",
            "Epoch 12/50, Train Loss: 0.1327\n",
            "Epoch 13/50, Train Loss: 0.1390\n",
            "Epoch 14/50, Train Loss: 0.1187\n",
            "Epoch 15/50, Train Loss: 0.0980, Val Loss: 0.0776\n",
            "Epoch 16/50, Train Loss: 0.0876\n",
            "Epoch 17/50, Train Loss: 0.0849\n",
            "Epoch 18/50, Train Loss: 0.0794\n",
            "Epoch 19/50, Train Loss: 0.0756\n",
            "Epoch 20/50, Train Loss: 0.0729, Val Loss: 0.0899\n",
            "Epoch 21/50, Train Loss: 0.0708\n",
            "Epoch 22/50, Train Loss: 0.0678\n",
            "Epoch 23/50, Train Loss: 0.0658\n",
            "Epoch 24/50, Train Loss: 0.0641\n",
            "Epoch 25/50, Train Loss: 0.0624, Val Loss: 0.0775\n",
            "Epoch 26/50, Train Loss: 0.0608\n",
            "Epoch 27/50, Train Loss: 0.0597\n",
            "Epoch 28/50, Train Loss: 0.0580\n",
            "Epoch 29/50, Train Loss: 0.0566\n",
            "Epoch 30/50, Train Loss: 0.0553, Val Loss: 0.0745\n",
            "Epoch 31/50, Train Loss: 0.0545\n",
            "Epoch 32/50, Train Loss: 0.0532\n",
            "Epoch 33/50, Train Loss: 0.0524\n",
            "Epoch 34/50, Train Loss: 0.0508\n",
            "Epoch 35/50, Train Loss: 0.0499, Val Loss: 0.0707\n",
            "Epoch 36/50, Train Loss: 0.0489\n",
            "Epoch 37/50, Train Loss: 0.0480\n",
            "Epoch 38/50, Train Loss: 0.0469\n",
            "Epoch 39/50, Train Loss: 0.0457\n",
            "Epoch 40/50, Train Loss: 0.0453, Val Loss: 0.0673\n",
            "Epoch 41/50, Train Loss: 0.0440\n",
            "Epoch 42/50, Train Loss: 0.0431\n",
            "Epoch 43/50, Train Loss: 0.0428\n",
            "Epoch 44/50, Train Loss: 0.0421\n",
            "Epoch 45/50, Train Loss: 0.0406, Val Loss: 0.0619\n",
            "Epoch 46/50, Train Loss: 0.0400\n",
            "Epoch 47/50, Train Loss: 0.0404\n",
            "Epoch 48/50, Train Loss: 0.0385\n",
            "Epoch 49/50, Train Loss: 0.0376\n",
            "Epoch 50/50, Train Loss: 0.0367, Val Loss: 0.0595\n",
            "\n",
            "Training Architecture 3 with Batch Size: 64\n",
            "Epoch 1/50, Train Loss: 3.5171\n",
            "Epoch 2/50, Train Loss: 3.2318\n",
            "Epoch 3/50, Train Loss: 2.9514\n",
            "Epoch 4/50, Train Loss: 2.6396\n",
            "Epoch 5/50, Train Loss: 2.2848, Val Loss: 2.3570\n",
            "Epoch 6/50, Train Loss: 1.9398\n",
            "Epoch 7/50, Train Loss: 1.5381\n",
            "Epoch 8/50, Train Loss: 1.1364\n",
            "Epoch 9/50, Train Loss: 0.7632\n",
            "Epoch 10/50, Train Loss: 0.4510, Val Loss: 0.4614\n",
            "Epoch 11/50, Train Loss: 0.2499\n",
            "Epoch 12/50, Train Loss: 0.2016\n",
            "Epoch 13/50, Train Loss: 0.2566\n",
            "Epoch 14/50, Train Loss: 0.3330\n",
            "Epoch 15/50, Train Loss: 0.3439, Val Loss: 0.1968\n",
            "Epoch 16/50, Train Loss: 0.2775\n",
            "Epoch 17/50, Train Loss: 0.1921\n",
            "Epoch 18/50, Train Loss: 0.1285\n",
            "Epoch 19/50, Train Loss: 0.1019\n",
            "Epoch 20/50, Train Loss: 0.1055, Val Loss: 0.1638\n",
            "Epoch 21/50, Train Loss: 0.1192\n",
            "Epoch 22/50, Train Loss: 0.1299\n",
            "Epoch 23/50, Train Loss: 0.1291\n",
            "Epoch 24/50, Train Loss: 0.1200\n",
            "Epoch 25/50, Train Loss: 0.1035, Val Loss: 0.1283\n",
            "Epoch 26/50, Train Loss: 0.0913\n",
            "Epoch 27/50, Train Loss: 0.0815\n",
            "Epoch 28/50, Train Loss: 0.0798\n",
            "Epoch 29/50, Train Loss: 0.0794\n",
            "Epoch 30/50, Train Loss: 0.0803, Val Loss: 0.0955\n",
            "Epoch 31/50, Train Loss: 0.0787\n",
            "Epoch 32/50, Train Loss: 0.0749\n",
            "Epoch 33/50, Train Loss: 0.0704\n",
            "Epoch 34/50, Train Loss: 0.0670\n",
            "Epoch 35/50, Train Loss: 0.0658, Val Loss: 0.1054\n",
            "Epoch 36/50, Train Loss: 0.0650\n",
            "Epoch 37/50, Train Loss: 0.0648\n",
            "Epoch 38/50, Train Loss: 0.0634\n",
            "Epoch 39/50, Train Loss: 0.0616\n",
            "Epoch 40/50, Train Loss: 0.0602, Val Loss: 0.0990\n",
            "Epoch 41/50, Train Loss: 0.0587\n",
            "Epoch 42/50, Train Loss: 0.0570\n",
            "Epoch 43/50, Train Loss: 0.0561\n",
            "Epoch 44/50, Train Loss: 0.0553\n",
            "Epoch 45/50, Train Loss: 0.0543, Val Loss: 0.0913\n",
            "Epoch 46/50, Train Loss: 0.0533\n",
            "Epoch 47/50, Train Loss: 0.0523\n",
            "Epoch 48/50, Train Loss: 0.0514\n",
            "Epoch 49/50, Train Loss: 0.0507\n",
            "Epoch 50/50, Train Loss: 0.0499, Val Loss: 0.0897\n",
            "\n",
            "Training Architecture 4 with Batch Size: 16\n",
            "Epoch 1/50, Train Loss: 3.3114\n",
            "Epoch 2/50, Train Loss: 0.8127\n",
            "Epoch 3/50, Train Loss: 0.5841\n",
            "Epoch 4/50, Train Loss: 0.6178\n",
            "Epoch 5/50, Train Loss: 0.6109, Val Loss: 0.6952\n",
            "Epoch 6/50, Train Loss: 0.5872\n",
            "Epoch 7/50, Train Loss: 0.5752\n",
            "Epoch 8/50, Train Loss: 0.5762\n",
            "Epoch 9/50, Train Loss: 0.5772\n",
            "Epoch 10/50, Train Loss: 0.5725, Val Loss: 0.6888\n",
            "Epoch 11/50, Train Loss: 0.5661\n",
            "Epoch 12/50, Train Loss: 0.5563\n",
            "Epoch 13/50, Train Loss: 0.5424\n",
            "Epoch 14/50, Train Loss: 0.5239\n",
            "Epoch 15/50, Train Loss: 0.4870, Val Loss: 0.5658\n",
            "Epoch 16/50, Train Loss: 0.4312\n",
            "Epoch 17/50, Train Loss: 0.3371\n",
            "Epoch 18/50, Train Loss: 0.2075\n",
            "Epoch 19/50, Train Loss: 0.1571\n",
            "Epoch 20/50, Train Loss: 0.1148, Val Loss: 0.1603\n",
            "Epoch 21/50, Train Loss: 0.0693\n",
            "Epoch 22/50, Train Loss: 0.0444\n",
            "Epoch 23/50, Train Loss: 0.0278\n",
            "Epoch 24/50, Train Loss: 0.0159\n",
            "Epoch 25/50, Train Loss: 0.0089, Val Loss: 0.0343\n",
            "Epoch 26/50, Train Loss: 0.0040\n",
            "Epoch 27/50, Train Loss: 0.0021\n",
            "Epoch 28/50, Train Loss: 0.0010\n",
            "Epoch 29/50, Train Loss: 0.0004\n",
            "Epoch 30/50, Train Loss: 0.0003, Val Loss: 0.0285\n",
            "Epoch 31/50, Train Loss: 0.0003\n",
            "Epoch 32/50, Train Loss: 0.0002\n",
            "Epoch 33/50, Train Loss: 0.0002\n",
            "Epoch 34/50, Train Loss: 0.0002\n",
            "Epoch 35/50, Train Loss: 0.0002, Val Loss: 0.0303\n",
            "Epoch 36/50, Train Loss: 0.0002\n",
            "Epoch 37/50, Train Loss: 0.0002\n",
            "Epoch 38/50, Train Loss: 0.0002\n",
            "Epoch 39/50, Train Loss: 0.0001\n",
            "Epoch 40/50, Train Loss: 0.0001, Val Loss: 0.0325\n",
            "Epoch 41/50, Train Loss: 0.0001\n",
            "Epoch 42/50, Train Loss: 0.0001\n",
            "Epoch 43/50, Train Loss: 0.0001\n",
            "Epoch 44/50, Train Loss: 0.0001\n",
            "Epoch 45/50, Train Loss: 0.0001, Val Loss: 0.0340\n",
            "Epoch 46/50, Train Loss: 0.0001\n",
            "Epoch 47/50, Train Loss: 0.0001\n",
            "Epoch 48/50, Train Loss: 0.0001\n",
            "Epoch 49/50, Train Loss: 0.0001\n",
            "Epoch 50/50, Train Loss: 0.0001, Val Loss: 0.0357\n",
            "\n",
            "Training Architecture 4 with Batch Size: 32\n",
            "Epoch 1/50, Train Loss: 4.5974\n",
            "Epoch 2/50, Train Loss: 2.7405\n",
            "Epoch 3/50, Train Loss: 1.3938\n",
            "Epoch 4/50, Train Loss: 0.8566\n",
            "Epoch 5/50, Train Loss: 0.6455, Val Loss: 0.7338\n",
            "Epoch 6/50, Train Loss: 0.5844\n",
            "Epoch 7/50, Train Loss: 0.5773\n",
            "Epoch 8/50, Train Loss: 0.5892\n",
            "Epoch 9/50, Train Loss: 0.5968\n",
            "Epoch 10/50, Train Loss: 0.5992, Val Loss: 0.6987\n",
            "Epoch 11/50, Train Loss: 0.5966\n",
            "Epoch 12/50, Train Loss: 0.5905\n",
            "Epoch 13/50, Train Loss: 0.5862\n",
            "Epoch 14/50, Train Loss: 0.5793\n",
            "Epoch 15/50, Train Loss: 0.5781, Val Loss: 0.6936\n",
            "Epoch 16/50, Train Loss: 0.5774\n",
            "Epoch 17/50, Train Loss: 0.5778\n",
            "Epoch 18/50, Train Loss: 0.5764\n",
            "Epoch 19/50, Train Loss: 0.5771\n",
            "Epoch 20/50, Train Loss: 0.5767, Val Loss: 0.6991\n",
            "Epoch 21/50, Train Loss: 0.5766\n",
            "Epoch 22/50, Train Loss: 0.5765\n",
            "Epoch 23/50, Train Loss: 0.5761\n",
            "Epoch 24/50, Train Loss: 0.5758\n",
            "Epoch 25/50, Train Loss: 0.5753, Val Loss: 0.6948\n",
            "Epoch 26/50, Train Loss: 0.5749\n",
            "Epoch 27/50, Train Loss: 0.5745\n",
            "Epoch 28/50, Train Loss: 0.5740\n",
            "Epoch 29/50, Train Loss: 0.5735\n",
            "Epoch 30/50, Train Loss: 0.5726, Val Loss: 0.6906\n",
            "Epoch 31/50, Train Loss: 0.5722\n",
            "Epoch 32/50, Train Loss: 0.5712\n",
            "Epoch 33/50, Train Loss: 0.5701\n",
            "Epoch 34/50, Train Loss: 0.5686\n",
            "Epoch 35/50, Train Loss: 0.5667, Val Loss: 0.6830\n",
            "Epoch 36/50, Train Loss: 0.5644\n",
            "Epoch 37/50, Train Loss: 0.5611\n",
            "Epoch 38/50, Train Loss: 0.5573\n",
            "Epoch 39/50, Train Loss: 0.5514\n",
            "Epoch 40/50, Train Loss: 0.5468, Val Loss: 0.6564\n",
            "Epoch 41/50, Train Loss: 0.5355\n",
            "Epoch 42/50, Train Loss: 0.5248\n",
            "Epoch 43/50, Train Loss: 0.5110\n",
            "Epoch 44/50, Train Loss: 0.4887\n",
            "Epoch 45/50, Train Loss: 0.4667, Val Loss: 0.5494\n",
            "Epoch 46/50, Train Loss: 0.4276\n",
            "Epoch 47/50, Train Loss: 0.3877\n",
            "Epoch 48/50, Train Loss: 0.3400\n",
            "Epoch 49/50, Train Loss: 0.2959\n",
            "Epoch 50/50, Train Loss: 0.2477, Val Loss: 0.2966\n",
            "\n",
            "Training Architecture 4 with Batch Size: 64\n",
            "Epoch 1/50, Train Loss: 2.8355\n",
            "Epoch 2/50, Train Loss: 2.2067\n",
            "Epoch 3/50, Train Loss: 1.4823\n",
            "Epoch 4/50, Train Loss: 0.8959\n",
            "Epoch 5/50, Train Loss: 0.6165, Val Loss: 0.6676\n",
            "Epoch 6/50, Train Loss: 0.5465\n",
            "Epoch 7/50, Train Loss: 0.5553\n",
            "Epoch 8/50, Train Loss: 0.5992\n",
            "Epoch 9/50, Train Loss: 0.6220\n",
            "Epoch 10/50, Train Loss: 0.6166, Val Loss: 0.6696\n",
            "Epoch 11/50, Train Loss: 0.5814\n",
            "Epoch 12/50, Train Loss: 0.5205\n",
            "Epoch 13/50, Train Loss: 0.4434\n",
            "Epoch 14/50, Train Loss: 0.3532\n",
            "Epoch 15/50, Train Loss: 0.2839, Val Loss: 0.3295\n",
            "Epoch 16/50, Train Loss: 0.2498\n",
            "Epoch 17/50, Train Loss: 0.2385\n",
            "Epoch 18/50, Train Loss: 0.2248\n",
            "Epoch 19/50, Train Loss: 0.2038\n",
            "Epoch 20/50, Train Loss: 0.1759, Val Loss: 0.2058\n",
            "Epoch 21/50, Train Loss: 0.1555\n",
            "Epoch 22/50, Train Loss: 0.1379\n",
            "Epoch 23/50, Train Loss: 0.1315\n",
            "Epoch 24/50, Train Loss: 0.1280\n",
            "Epoch 25/50, Train Loss: 0.1252, Val Loss: 0.1181\n",
            "Epoch 26/50, Train Loss: 0.1194\n",
            "Epoch 27/50, Train Loss: 0.1081\n",
            "Epoch 28/50, Train Loss: 0.0964\n",
            "Epoch 29/50, Train Loss: 0.0873\n",
            "Epoch 30/50, Train Loss: 0.0825, Val Loss: 0.1172\n",
            "Epoch 31/50, Train Loss: 0.0741\n",
            "Epoch 32/50, Train Loss: 0.0620\n",
            "Epoch 33/50, Train Loss: 0.0526\n",
            "Epoch 34/50, Train Loss: 0.0460\n",
            "Epoch 35/50, Train Loss: 0.0390, Val Loss: 0.0491\n",
            "Epoch 36/50, Train Loss: 0.0325\n",
            "Epoch 37/50, Train Loss: 0.0277\n",
            "Epoch 38/50, Train Loss: 0.0236\n",
            "Epoch 39/50, Train Loss: 0.0201\n",
            "Epoch 40/50, Train Loss: 0.0169, Val Loss: 0.0228\n",
            "Epoch 41/50, Train Loss: 0.0145\n",
            "Epoch 42/50, Train Loss: 0.0123\n",
            "Epoch 43/50, Train Loss: 0.0108\n",
            "Epoch 44/50, Train Loss: 0.0096\n",
            "Epoch 45/50, Train Loss: 0.0083, Val Loss: 0.0204\n",
            "Epoch 46/50, Train Loss: 0.0072\n",
            "Epoch 47/50, Train Loss: 0.0061\n",
            "Epoch 48/50, Train Loss: 0.0056\n",
            "Epoch 49/50, Train Loss: 0.0048\n",
            "Epoch 50/50, Train Loss: 0.0041, Val Loss: 0.0189\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation & Analysis (4 marks)"
      ],
      "metadata": {
        "id": "vxVGUr_ksFZc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Set Evaluation (3 marks)\n",
        "\n",
        "Evaluate the model on the test set\n",
        "\n",
        "Report:\n",
        "1. Mean Squared Error (MSE)\n",
        "2. Mean Absolute Error (MAE)"
      ],
      "metadata": {
        "id": "YSE0EQVMsID8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "test_dataset_tensor = TensorDataset(test_X_scaled, test_y)\n",
        "test_loader_eval = DataLoader(test_dataset_tensor, batch_size=64, shuffle=False)\n",
        "\n",
        "for arch_name, arch_results in results.items():\n",
        "    for batch_size, data in arch_results.items():\n",
        "        model = data['model']\n",
        "        model.eval()\n",
        "        all_predictions = []\n",
        "        all_targets = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, targets in test_loader_eval:\n",
        "                outputs = model(inputs)\n",
        "                all_predictions.append(outputs.cpu().numpy())\n",
        "                all_targets.append(targets.cpu().numpy())\n",
        "        predictions = np.concatenate(all_predictions, axis=0)\n",
        "        actuals = np.concatenate(all_targets, axis=0)\n",
        "        mse = mean_squared_error(actuals, predictions)\n",
        "        mae = mean_absolute_error(actuals, predictions)\n",
        "        data['test_mse'] = mse\n",
        "        data['test_mae'] = mae\n",
        "        print(f\"\\n{arch_name} (Batch Size: {batch_size}) - Test MSE: {mse:.4f}, Test MAE: {mae:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCotpWYfsF_F",
        "outputId": "f40faa89-910f-4c32-a50d-b4f59df5708a"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Architecture 1 (Batch Size: 16) - Test MSE: 0.0308, Test MAE: 0.1412\n",
            "\n",
            "Architecture 1 (Batch Size: 32) - Test MSE: 0.0612, Test MAE: 0.1927\n",
            "\n",
            "Architecture 1 (Batch Size: 64) - Test MSE: 0.0665, Test MAE: 0.1896\n",
            "\n",
            "Architecture 2 (Batch Size: 16) - Test MSE: 0.0566, Test MAE: 0.1775\n",
            "\n",
            "Architecture 2 (Batch Size: 32) - Test MSE: 0.0577, Test MAE: 0.1945\n",
            "\n",
            "Architecture 2 (Batch Size: 64) - Test MSE: 0.0745, Test MAE: 0.2157\n",
            "\n",
            "Architecture 3 (Batch Size: 16) - Test MSE: 0.0321, Test MAE: 0.1253\n",
            "\n",
            "Architecture 3 (Batch Size: 32) - Test MSE: 0.0520, Test MAE: 0.1786\n",
            "\n",
            "Architecture 3 (Batch Size: 64) - Test MSE: 0.0694, Test MAE: 0.2066\n",
            "\n",
            "Architecture 4 (Batch Size: 16) - Test MSE: 0.0032, Test MAE: 0.0161\n",
            "\n",
            "Architecture 4 (Batch Size: 32) - Test MSE: 0.2204, Test MAE: 0.3444\n",
            "\n",
            "Architecture 4 (Batch Size: 64) - Test MSE: 0.0097, Test MAE: 0.0511\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate the results. (1 mark)"
      ],
      "metadata": {
        "id": "DPWsJ_WIsK6M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Analysis of Model Performance**\n",
        "\n",
        "Review of Test MSE and Test MAE:\n",
        "\n",
        "* Architecture 1 (ReLU, ReLU):\n",
        "\n",
        "    * Batch Size 16: MSE = 0.0308, MAE = 0.1412\n",
        "    * Batch Size 32: MSE = 0.0612, MAE = 0.1927\n",
        "    * Batch Size 64: MSE = 0.0665, MAE = 0.1896\n",
        "\n",
        "\n",
        "* Architecture 2 (Tanh, Tanh):\n",
        "\n",
        "    * Batch Size 16: MSE = 0.0566, MAE = 0.1775\n",
        "    * Batch Size 32: MSE = 0.0577, MAE = 0.1945\n",
        "    * Batch Size 64: MSE = 0.0745, MAE = 0.2157\n",
        "\n",
        "* Architecture 3 (ReLU, ReLU, ReLU):\n",
        "\n",
        "    * Batch Size 16: MSE = 0.0321, MAE = 0.1253\n",
        "    * Batch Size 32: MSE = 0.0520, MAE = 0.1786\n",
        "    * Batch Size 64: MSE = 0.0694, MAE = 0.2066\n",
        "\n",
        "* Architecture 4 (ReLU, LeakyReLU, Tanh, Sigmoid):\n",
        "\n",
        "    * Batch Size 16: MSE = 0.0032, MAE = 0.0162\n",
        "    * Batch Size 32: MSE = 0.2204, MAE = 0.3444\n",
        "    * Batch Size 64: MSE = 0.0097, MAE = 0.0511\n",
        "\n",
        "**Conclusion**: Architecture 4 with batch size 16 is performing the best for this example."
      ],
      "metadata": {
        "id": "eYz5BWzB4ReV"
      }
    }
  ]
}