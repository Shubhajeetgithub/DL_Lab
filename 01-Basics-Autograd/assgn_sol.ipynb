{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Name: Shubhajeet Das <br />\n",
        "Roll No.: 24AI10013 <br />\n",
        "Date: 6 Jan 2026"
      ],
      "metadata": {
        "id": "cdgh2aAbn-wP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Deep Learning (PyTorch): Assignment 1"
      ],
      "metadata": {
        "id": "4pfWCCi_E046"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create A 3-dimensional tensor X representing a batch of padded sequences:\n",
        "\n",
        "[Marks 1]\n",
        "\n",
        "* Batch size B = 6\n",
        "\n",
        "* Maximum sequence length T = 10\n",
        "\n",
        "* Feature dimension F = 8\n",
        "\n",
        "* Values must be sampled from a standard normal distribution\n",
        "\n",
        "* The tensor must participate in backpropagation\n",
        "\n"
      ],
      "metadata": {
        "id": "ukaBREXAE1RY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "X = torch.randn(6, 10, 8, requires_grad=True)\n",
        "print(\"Tensor X created with shape:\", X.shape)\n",
        "print(\"Requires gradient:\", X.requires_grad)"
      ],
      "metadata": {
        "id": "qhwFta2ZE5Hg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6ce0f8ef-0f2d-4e7d-8977-7666344205c1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor X created with shape: torch.Size([6, 10, 8])\n",
            "Requires gradient: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Create a mask of shape (6, 10) containing 0, 1 only.\n",
        "\n",
        "[Marks 1]\n"
      ],
      "metadata": {
        "id": "9zBEvbfZG_FB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mask = torch.randint(0, 2, (6, 10))\n",
        "print(\"Mask created with shape:\", mask.shape)\n",
        "print(\"Mask content sample:\\n\", mask)"
      ],
      "metadata": {
        "id": "MwGy947KHq3R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddb4d233-0dbf-45e1-d646-346247f6c21d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mask created with shape: torch.Size([6, 10])\n",
            "Mask content sample:\n",
            " tensor([[1, 0, 1, 1, 1, 1, 1, 0, 0, 0],\n",
            "        [0, 1, 1, 1, 0, 0, 1, 0, 0, 0],\n",
            "        [1, 0, 0, 0, 1, 1, 0, 0, 0, 1],\n",
            "        [1, 0, 1, 0, 0, 0, 0, 0, 1, 1],\n",
            "        [0, 1, 1, 0, 1, 1, 0, 1, 0, 1],\n",
            "        [0, 0, 0, 0, 0, 1, 0, 1, 1, 0]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Masked Reduction\n",
        "[marks 7]\n",
        "\n",
        "Compute a tensor H ∈ ℝ^{B×F}:\n",
        "- Must rely on broadcasting + reduction\n",
        "- Operation must be numerically stable"
      ],
      "metadata": {
        "id": "5E27skJ3FZ5w"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "J5GPMfgz65hr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a14a2c74-b39d-4bd0-bf31-f71fed1006c6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor H created with shape: torch.Size([6, 8])\n",
            "H sample:\n",
            " tensor([[-0.2134, -0.7200,  0.8103,  0.1301,  1.0648, -0.9419,  0.1278,  0.7643],\n",
            "        [-0.3521,  1.2659,  0.5668,  0.1090, -0.7020,  0.4657,  0.0454,  0.5968],\n",
            "        [ 1.0081, -0.1255,  0.3033, -0.9279, -0.3497,  0.7482, -0.4652,  0.4079],\n",
            "        [ 0.5772, -0.2298,  0.4986,  0.1489, -0.2169, -0.2485, -0.2643,  0.4779],\n",
            "        [ 0.3829, -0.7855,  0.3554,  0.3611, -0.3511,  0.2494,  0.0743,  0.2733],\n",
            "        [-0.7410, -0.3905,  0.4918, -0.4858, -0.5172,  0.2507, -0.2818,  0.8785]],\n",
            "       grad_fn=<DivBackward0>)\n"
          ]
        }
      ],
      "source": [
        "mask_unsqueezed = mask.unsqueeze(-1)\n",
        "masked_X = X * mask_unsqueezed\n",
        "sum_masked_X = masked_X.sum(dim=1)\n",
        "lengths = mask.sum(dim=1, keepdim=True)\n",
        "H = sum_masked_X / (lengths + 1e-9)\n",
        "\n",
        "print(\"Tensor H created with shape:\", H.shape)\n",
        "print(\"H sample:\\n\", H)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Matrix Multiplication"
      ],
      "metadata": {
        "id": "Y9-bCK2DIOG4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## You are given (given Piece of code.)\n",
        "import torch\n",
        "X = torch.randn(32, 64, requires_grad=True)\n",
        "W = torch.randn(64, 16, requires_grad=True)\n",
        "b = torch.randn(16, requires_grad=True)\n"
      ],
      "metadata": {
        "id": "AYXhhmK5IgQw"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compute:\n",
        "[Marks 2]\n",
        "\n",
        "Y=XW+b\n",
        "\n",
        "such that broadcasting of b happens implicitly."
      ],
      "metadata": {
        "id": "wmiOQFItIzkA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Y = X @ W + b\n",
        "print(\"Tensor Y created with shape:\", Y.shape)\n",
        "print(\"Y sample (first row):\\n\", Y[0])"
      ],
      "metadata": {
        "id": "t-cKIrwRI4jK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0772e100-0c44-46d3-a662-228e7a099852"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tensor Y created with shape: torch.Size([32, 16])\n",
            "Y sample (first row):\n",
            " tensor([  1.4631,  -9.6281,   7.0019,   6.3223,   2.9396,   4.1752,   5.8190,\n",
            "         -5.9924,  -2.6496, -21.5498,   7.0579,   1.7749,  13.6921,   3.0240,\n",
            "         -7.9628,   4.7901], grad_fn=<SelectBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compute a scalar loss:\n",
        "[Marks 1]\n",
        "\n",
        "L=∑Y^2"
      ],
      "metadata": {
        "id": "Myi560i1JC0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "L = (Y ** 2).sum()\n",
        "print(\"Scalar loss L computed:\", L)"
      ],
      "metadata": {
        "id": "0ahWDawgIpgT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03136e88-84be-477f-9cd4-0b31b35f9746"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Scalar loss L computed: tensor(33341.1914, grad_fn=<SumBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Objective\n",
        "[Marks 3]\n",
        "\n",
        "* What is the shape of Y?\n",
        "\n",
        "* Which dimension(s) does PyTorch reduce over during backpropagation into b?\n",
        "\n",
        "* Why would X @ W.T silently fail or produce wrong shapes?\n"
      ],
      "metadata": {
        "id": "m45QeUPuJIO5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **What is the shape of Y?**\n",
        "    The shape of `Y` is **(32, 16)**.\n",
        "\n",
        "*   **Which dimension(s) does PyTorch reduce over during backpropagation into b?**\n",
        "    When computing `Y = X @ W + b`, `b` is effectively added to each row of `X @ W`. If `Y` has shape (B, F) and `b` has shape (F,), then during backpropagation, `dL/db` is obtained by summing `dL/dY` along the batch dimension (dimension 0). This is because `b` affects each row of `Y` identically, so its gradient accumulates contributions from all rows.\n",
        "    Therefore, PyTorch reduces over **dimension 0 (the batch dimension)** during backpropagation into `b`.\n",
        "\n",
        "*   **Why would X @ W.T silently fail or produce wrong shapes?**\n",
        "    `X` has shape (32, 64).\n",
        "    `W` has shape (64, 16).\n",
        "    `W.T` (transpose of W) would have shape (16, 64).\n",
        "\n",
        "    For matrix multiplication `A @ B`, the inner dimensions must match.\n",
        "    In `X @ W.T`:\n",
        "    - `X` has shape (32, **64**)\n",
        "    - `W.T` has shape (**16**, 64)\n",
        "    The inner dimensions are 64 and 16, which **do not match**. Therefore, `X @ W.T` would **silently fail** with a runtime error (e.g., `RuntimeError: mat1 and mat2 shapes cannot be multiplied`)."
      ],
      "metadata": {
        "id": "FrcFK65BJaTS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Backpropagation"
      ],
      "metadata": {
        "id": "wwcx8TApJb1x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code\n",
        "\n",
        "```python\n",
        "import torch\n",
        "\n",
        "B = 4  # batch_size\n",
        "\n",
        "X = torch.randn(B, 6, requires_grad=True)\n",
        "\n",
        "W1 = torch.randn(6, 5, requires_grad=True)\n",
        "b1 = torch.randn(5, requires_grad=True)\n",
        "\n",
        "W2 = torch.randn(5, 3, requires_grad=True)\n",
        "b2 = torch.randn(3, requires_grad=True)\n",
        "\n",
        "H = X @ W1 + b1            #1\n",
        "A = torch.relu(H.detach()) #2    \n",
        "Y = A @ W2 + b2            #3\n",
        "L = (Y ** 2).mean()        #4\n",
        "```"
      ],
      "metadata": {
        "id": "4-iAJldyKXzB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Code Reading,\n",
        "[Marks 1]\n",
        "\n",
        "One of the given 4 lines (\\#1, \\#2, \\#3, \\#4) is wrong. Identify that line and justify that line."
      ],
      "metadata": {
        "id": "KK9MZn4HLIa5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**The wrong line is: `#2 A = torch.relu(H.detach())`**\n",
        "\n",
        "**Justification:**\n",
        "The `detach()` method creates a new tensor that is detached from the current computational graph. This means that any operations performed *after* `H.detach()` will not have their gradients propagated back through `H` (and consequently, not through `X`, `W1`, or `b1`). By detaching `H` before applying the `relu` activation and feeding it into the next layer, we are effectively preventing gradient flow from `Y` and `L` back to `H`, `W1`, `b1`, and `X`."
      ],
      "metadata": {
        "id": "_I0LT-fpLflQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Without running the code(after correcting the wrong line), determine the exact shape of the following tensors:\n",
        "[Marks 2]\n",
        "\n",
        "- H -> (4, 5)\n",
        "\n",
        "- A -> (4, 5)\n",
        "\n",
        "- Y -> (4, 3)\n",
        "\n",
        "- L -> () scalar"
      ],
      "metadata": {
        "id": "HFJakBXtKiap"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### After Back Propagation,\n",
        "[Marks 2]\n",
        "``` python\n",
        "L.backward()\n",
        "```\n",
        "After applyng back-propagate\n",
        "State the shape of the gradient for each of the following:\n",
        "\n",
        "∂L / ∂H\n",
        "\n",
        "∂L / ∂W1\n",
        "\n",
        "∂L / ∂b1\n",
        "\n",
        "∂L / ∂X"
      ],
      "metadata": {
        "id": "eWEwvM89LmkR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "B = 4  # batch_size\n",
        "\n",
        "X = torch.randn(B, 6, requires_grad=True)\n",
        "\n",
        "W1 = torch.randn(6, 5, requires_grad=True)\n",
        "b1 = torch.randn(5, requires_grad=True)\n",
        "\n",
        "W2 = torch.randn(5, 3, requires_grad=True)\n",
        "b2 = torch.randn(3, requires_grad=True)\n",
        "\n",
        "H = X @ W1 + b1\n",
        "A = torch.relu(H)\n",
        "Y = A @ W2 + b2\n",
        "L = (Y ** 2).mean()\n",
        "L.backward()"
      ],
      "metadata": {
        "id": "tIezf4_3KmKh"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **∂L / ∂H**: The gradient `dL/dH` will have the same shape as `H`. From the previous step, `H` has shape `(4, 5)`. Therefore, `∂L / ∂H` will have shape **(4, 5)**.\n",
        "\n",
        "*   **∂L / ∂W1**: The gradient `dL/dW1` will have the same shape as `W1`. `W1` has shape `(6, 5)`. Therefore, `∂L / ∂W1` will have shape **(6, 5)**.\n",
        "\n",
        "*   **∂L / ∂b1**: The gradient `dL/db1` will have the same shape as `b1`. `b1` has shape `(5,)`. Therefore, `∂L / ∂b1` will have shape **(5,)**.\n",
        "\n",
        "*   **∂L / ∂X**: The gradient `dL/dX` will have the same shape as `X`. `X` has shape `(4, 6)`. Therefore, `∂L / ∂X` will have shape **(4, 6)**."
      ],
      "metadata": {
        "id": "nL9fExfa3HS4"
      }
    }
  ]
}